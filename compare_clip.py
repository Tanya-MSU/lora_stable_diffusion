from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch
import os
from pathlib import Path

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
prompt = "an artwork in baroque style"
before_dir = Path("outputs/before_lora")
after_dir = Path("outputs/after_lora")
device = "cuda" if torch.cuda.is_available() else "cpu"

# === –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ===
def get_latest_image(folder):
    images = sorted(folder.glob("*.png"), key=os.path.getmtime)
    return images[-1] if images else None

before_path = get_latest_image(before_dir)
after_path = get_latest_image(after_dir)

if not before_path or not after_path:
    raise FileNotFoundError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")

# === –ó–∞–≥—Ä—É–∑–∫–∞ CLIP –º–æ–¥–µ–ª–∏ ===
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è CLIP similarity ===
def compute_similarity(image_path):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(text=[prompt], images=image, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    return torch.cosine_similarity(outputs.image_embeds, outputs.text_embeds).item()

# === –í—ã—á–∏—Å–ª—è–µ–º –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º ===
before_sim = compute_similarity(before_path)
after_sim = compute_similarity(after_path)

print(f"\nBEFORE ({before_path.name}): {round(before_sim, 4)}")
print(f"AFTER  ({after_path.name}): {round(after_sim, 4)}")

improvement = after_sim - before_sim
print("\nüìä –£–ª—É—á—à–µ–Ω–∏–µ:", round(improvement, 4))
if improvement > 0:
    print("‚úÖ LoRA —É–ª—É—á—à–∏–ª–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç—É.")
elif improvement < 0:
    print("‚ùå LoRA —Å–¥–µ–ª–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ö—É–∂–µ.")
else:
    print("‚ûñ –†–∞–∑–Ω–∏—Ü—ã –Ω–µ—Ç.")
